{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The code for DDPG was implemented with the help of udacity lecture material\n",
    " - Also referred ShagtongZhang Deep reinforcement learning library `https://github.com/ShangtongZhang/DeepRL` to debug my model\n",
    " \n",
    " \n",
    " The Idea is was to build a multi-agent DDPG model that teaches the agents to take necessary actions (using its corresponding  actors) while the individual critic models observe the entire observation state (appended) only in training phase.\n",
    " It also partially follows the [Paper](https://arxiv.org/pdf/1706.02275.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MADDPG](./assets/maddpg_model.png \"MADDPG model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model built chose consists of Actor and Critic for both the Tennis agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters and hyperparameters\n",
    "\n",
    "\n",
    "- BUFFER_SIZE  # replay buffer size\n",
    "- BATCH_SIZE # minibatch size\n",
    "- GAMMA # discount factor\n",
    "- TAU # for soft update of target parameters\n",
    "- LR # learning rate \n",
    "- UPDATE_EVERY # how often to update the network\n",
    "\n",
    "\n",
    "| Parameters               | value                    |\n",
    "|--------------------------|--------------------------|\n",
    "| Buffer size              | 1e5                      |\n",
    "| batch size               | 128                      |\n",
    "| GAMMA                    | 0.99                     |\n",
    "| TAU                      | 5e-2                     |\n",
    "| LR                       | 5e-4                     |\n",
    "| Update every             | 4                        |\n",
    "| Ornsteinâ€“Uhlenbeck Noise | decay every 250 episodes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models used for agents\n",
    "![models](./assets/models.png \"models used\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For Actor\n",
    "\n",
    "Actor Net(\n",
    "  (fc1): Linear(in_features=48, out_features=256, bias=True)\n",
    "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
    "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For Critic\n",
    "\n",
    "Critic Net(\n",
    "  (fc1): Linear(in_features=52, out_features=256, bias=True)\n",
    "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
    "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots of Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![plotrewards](./assets/scores.png \"rewards plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment was solved in 2891 episodes! Average Score: 0.50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Will explore Soft Actor Critic (SAC) model that maximizes the entropy https://arxiv.org/pdf/1801.01290.pdf which is a promising alternative to DDPG\n",
    "- Twin Delayed DDPG would be a simple extension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
